{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual file used to scrape 1hr of raw data ('raw_data' folder) \n",
    "    # MongoDB ---> Pandas ---> .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from json import loads\n",
    "import datetime \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "# Charting-specific imports\n",
    "import matplotlib.finance\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.finance import candlestick_ohlc,candlestick2_ohlc\n",
    "from matplotlib.finance import volume_overlay,volume_overlay2\n",
    "from matplotlib.dates import  DateFormatter, epoch2num\n",
    "    # https://matplotlib.org/api/finance_api.html#module-matplotlib.finance\n",
    "\n",
    "# API-specific imports (local install)\n",
    "import gdax\n",
    "    # Python setup.py install with environment activaated to install/use\n",
    "    # Do not use default gdax pip install package - that version of the package is currently broken\n",
    "\n",
    "# Pymongo import for connection to local client DB\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Preprocessing Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from keras.utils import to_categorical \n",
    "\n",
    "# ML Imports \n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Embedding, Flatten\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "# autoSR() import requirements\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from pandas_datareader import data, wb\n",
    "\n",
    "###########################################################################\n",
    "### Force Keras/TF to use CPU backend when GPU present by setting:\n",
    "    # {'CPU' : 1, 'GPU' : 0}\n",
    "    \n",
    "#num_cores = 4\n",
    "#config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "        #inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "        #device_count = {'CPU' : 1, 'GPU' : 1})\n",
    "#session = tf.Session(config=config)\n",
    "#K.set_session(session)\n",
    "###########################################################################\n",
    "\n",
    "# Import to check check for GPU availability for tensorflow backend\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "# Boolean to drop existing mongo collection/scrape upon scrape() init\n",
    "dropFlag = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection establishment\n",
    "\n",
    "# Establish connection to GDAX public endpoint\n",
    "public_client = gdax.PublicClient()\n",
    "\n",
    "# Mongo database and collection specification:\n",
    "mongo_client = MongoClient('mongodb://localhost:27017/')\n",
    "db = mongo_client.btcusd_db\n",
    "btcusd_collection = db.btcusd_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to start scrape process from websocket to mongodb instance\n",
    "def scrape_start():\n",
    "    print(\"Starting scrape.....\")\n",
    "    # Drop existing collection from db if dropFlag == True (on new scrape):\n",
    "    if 'btcusd_db' in mongo_client.database_names() and dropFlag is True:\n",
    "        mongo_client['btcusd_db'].drop_collection('btcusd_collection')\n",
    "        #print(mongo_client.database_names())\n",
    "        #print(db.collection_names())\n",
    "        \n",
    "    # Start instance of websocket client for L2 Orderbook + L2 update data request and scrape\n",
    "    wsClient = gdax.WebsocketClient(url=\"wss://ws-feed.gdax.com\", \n",
    "                                products=[\"BTC-USD\"],\\\n",
    "                                message_type=\"subscribe\",\\\n",
    "                                channels =[\"level2\"],\\\n",
    "                                mongo_collection=btcusd_collection,\\\n",
    "                                should_print=False)\n",
    "    \n",
    "    # Save request open time and start websocket\n",
    "    time.sleep(4)\n",
    "    request_time_start=public_client.get_time()\n",
    "    print(\"Opening websocket connection.....\")\n",
    "    wsClient.start()\n",
    "    \n",
    "    # scrape_time is variable for time between websocket connection start and end\n",
    "        # Defined in seconds\n",
    "        # i.e. 600 seconds = scrape running for 10 minutes\n",
    "        # 3600 = one hour\n",
    "        # 36000 = ten hours\n",
    "    scrape_time = 36000\n",
    "    \n",
    "    time.sleep(scrape_time)\n",
    "    # Save request close time and close websocket\n",
    "    request_time_end=public_client.get_time()\n",
    "    wsClient.close()\n",
    "    print(\"Websocket connection closed.\")\n",
    "    \n",
    "    # Append request times for open/close of websocket stream to dataframe, save to csv\n",
    "    request_log_df = pd.DataFrame.from_dict({'request start':request_time_start,'request end':request_time_end},orient ='index')\n",
    "    request_log_df.to_csv(\"raw_data/request_log_10h.csv\",header=True,encoding='utf-8',index =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Set flag to true to drop existing mongodb collection (previously scraped data)\n",
    "dropFlag = True\n",
    "scrape_start()\n",
    "print(\"Scrape completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and parse data from Mongo into dataframes\n",
    "def load_parse():\n",
    "    \n",
    "    #Collection specification (in database)\n",
    "    input_data = db.btcusd_collection \n",
    "    \n",
    "    # Create individual dataframes for main response types: snapshot, l2update\n",
    "    snapshot = pd.DataFrame(list(input_data.find({'type':'snapshot'})))\n",
    "    l2update = pd.DataFrame(list(input_data.find({'type':'l2update'})))\n",
    "    \n",
    "    ### snapshot  response load and parse ###\n",
    "    \n",
    "    # Extract asks/bid individual column of array of arrays into lists\n",
    "    snapshot_asks = snapshot[['asks'][0]][0]\n",
    "    snapshot_bids = snapshot[['bids'][0]][0]\n",
    "    \n",
    "    # Convert list (of array of arrays) into dataframe\n",
    "    snapshot_asks_df =pd.DataFrame(snapshot_asks)\n",
    "    snapshot_bids_df =pd.DataFrame(snapshot_bids)\n",
    "    \n",
    "    # Rename columns to snapshot array format:\n",
    "        # snapshot array format: [price, size]\n",
    "            # [side, price, size] format for one-hot encoding\n",
    "        # Ask = sell price, bid = buy price\n",
    "    snapshot_asks_df.rename(columns ={0:'price',1:'size'}, inplace =True)\n",
    "    snapshot_bids_df.rename(columns ={0:'price',1:'size'}, inplace =True)\n",
    "    snapshot_asks_df['side'] = \"sell\"\n",
    "    snapshot_bids_df['side'] = \"buy\"\n",
    "    cols =['side','price','size']\n",
    "    snapshot_asks_df = snapshot_asks_df[cols]\n",
    "    snapshot_bids_df = snapshot_bids_df[cols]\n",
    "    \n",
    "    ### L2 update response load and parse ###\n",
    "    \n",
    "    # Restucture l2update to have [side,price,size] from 'changes' column\n",
    "    l2update_clean = l2update[['changes','time']]\n",
    "          \n",
    "    # Convert changes list of lists -> into array \n",
    "    l2_array = np.ravel(l2update_clean['changes']) \n",
    "    # Flatten the list and remove outer bracket:\n",
    "    flattened = [val for sublist in l2_array for val in sublist]\n",
    "        # Reference: https://stackoverflow.com/questions/11264684/flatten-list-of-lists?\n",
    "    # Convert back to dataframe and combine with timestamps from l2update:\n",
    "    changes_df= pd.DataFrame.from_records(flattened)\n",
    "    # Add time column back to L2 update dataframe\n",
    "    l2update_formatted = pd.concat([changes_df,l2update_clean['time']],1)\n",
    "    # Rename columns for [side, price, size]:\n",
    "    l2update_formatted.rename({0:\"side\",1:\"price\",2:\"size\"}, axis ='columns',inplace=True)\n",
    "    \n",
    "    # Save parsed data to csv (API -> Mongo -> Dataframe -> .csv)\n",
    "    #save_csv()\n",
    "    \n",
    "     # Save data to .csv format in raw_data folder\n",
    "    l2update_formatted.to_csv(\"raw_data/l2update_10h.csv\",header=True,encoding='utf-8',index =False)\n",
    "    snapshot_asks_df.to_csv(\"raw_data/snapshot_asks_10h.csv\",header=True,encoding='utf-8',index =False)\n",
    "    snapshot_bids_df.to_csv(\"raw_data/snapshot_bids_10h.csv\",header=True,encoding='utf-8',index =False)\n",
    "    #subscriptions.to_csv(\"raw_data/subscriptions.csv\",header=True,encoding='utf-8',index =False)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "load_parse()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
